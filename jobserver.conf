PLAY_HTTP_GUARDIAN_SECRET_KEY="AlertAccess1234"
PLAY_HTTP_TFA_KEY="TFAAlertAccess1234"
PLAY_HTTP_CSQ_KEY="CSQAlertAccess1234"
PLAY_HTTP_RNK_KEY="RNKAlertAccess1234"

ALERT_PRE_CONFIGURED_EMAILID="alertsupport@alertenterprise.com"

#Licence config
MAXKIOSKLICENSES="5"
VALIDATELICENSECONFIG="false"
LICENSEWARNINGDAYS=30

IDENTITYSOURCE="aeguardian"

#DB connectionpool/thread pool config
DB_DEFAULT_CONNECTIONPOOL=80
DB_LOGGING_CONNECTIONPOOL=10
DB_ANALYTICS_CONNECTIONPOOL=20
DB_EVENTS_CONNECTIONPOOL=30

DB_DEFAULT_DISPATCHER_THREAD_POOL=120
DB_LOGGING_DISPATCHER_THREAD_POOL=10
DB_ANALYTICS_DISPATCHER_THREAD_POOL=30

AKKA_ACTOR_THREAD_POOL=450

EXT_IDENTITY_API_THREAD_POOL=50
TIMELINE_THREAD_POOL=50
TRANSLATION_THREAD_POOL=50
ASSET_CACHE_API_THREAD_POOL=50
JOB_IDN_WRITER_THREAD_POOL=50
RULE_EXECUTION_THREAD_POOL=50
WORKFLOW_ACTION_THREAD_POOL=50
REQUEST_PROVISIONING_THREAD_POOL=50
WORKFLOW_JOB_THREAD_POOL=30
WORKFLOW_EMAIL_ACTION_THREAD_POOL=200
ACCESS_REQUEST_THREAD_POOL = 50

#Token config
AUTH_TOKEN_EXPIRY=120
REFRESH_TOKEN_EXPIRY=12000
TWOFACTORAUTH_TOKEN_EXPIRY_SECS=60
RESETAUTH_TOKEN_EXPIRY_SECS=240
AUTH_TOKEN_KIOSK_EXPIRY=43800
AUTH_TOKEN_INVITATION_EXPIRY=480

#Eventing ThreadPool Config
DB_EVENTS_CORE_POOL_SIZE=30
DB_EVENTS_QUEUE_SIZE=50
DB_EVENTS_MAX_POOL_SIZE=50
DB_EVENTS_READ_CORE_POOL_SIZE=30
DB_EVENTS_READ_QUEUE_SIZE=50
DB_EVENTS_READ_MAX_POOL_SIZE=50

#Play Evolution config
PLAY_EVOLUTIONS_ENABLED=false
APPLYEVOLUTIONS_DEFAULT=false
PLAY_EVOLUTIONS_AUTOAPPLY=false
PLAY_EVOLUTIONS_SCHEMA=aehsc

#AES Settings
AES_ENCRYPTION_PASSPHRASE="In the middle of difficulty lies opportunity"
AES_ENCRYPTION_ITERATIONCOUNT=65536
AES_ENCRYPTION_KEYLENGTH=256

#Kakfa Config
KAFKA_BOOTSTRAP_SERVERS="0.0.0.0:9092,0.0.0.0:9093"
KAFKA_CLIENT_ID="alert"

#Clamav settings
CLAMAV_SCANDISABLED=true
CLAMAV_CLAMD_HOST="localhost"
CLAMAV_CLAMD_PORT=3310

#Some Miscellaneous application configs
IDM_IMPORT_LIMIT=1000
IDM_IMPORT_MAPPING_SYSTEM="EXT_SYSTEM_FOR_DATA_SYNC"
IMPORT_JOB_CHUNKSIZE=10
WORKFLOW_JOB_CHUNKSIZE=2
USER_RECON_JOB_CHUNKSIZE=100
ENTITY_UPDATE_JOB_CHUNKSIZE=100
DISPLAY_STACKTRACE_RESPONSE="true"
TRANSLATION_DISABLED=true
ANALYTICS_UI_EXPORT_LIMIT=5000
ANALYTICS_JOB_EXPORT_LIMIT=10000
JAVA_OBJECT_TYPES_TIMELINE_MESSAGES=["String","Long","Integer"]
MAX_SIZE_LIMIT_FOR_UPLOAD=2097152

JOBSERVER_SCHEDULER_PRIMARY=true

#Database Time Zone
APP_DATABASE_TIMEZONE="UTC"

#External App URls
IDENTITY_INTEL_BASE_URL="http://localhost:5000"

MENU_CACHE_EXPIRY_SECS=3600
CUSTOM_FIELDS_CACHE_EXPIRY_SECS=3600

#Remove password from logs in publisher
TRIM_PASSWORD_LOGS=true

#Encrypted matching words in logs
RESTRICTED_WORDS_IN_LOGS=["PASSWORD"]

include "environment"
include "cache"

play.http.secret.key=${PLAY_HTTP_SECRET_KEY}
play.http.guardian.secret.key=${PLAY_HTTP_GUARDIAN_SECRET_KEY}
play.http.tfa.key=${PLAY_HTTP_TFA_KEY}
play.http.csq.key=${PLAY_HTTP_CSQ_KEY}
play.http.rnk.key=${PLAY_HTTP_RNK_KEY}

play.http.session.sameSite = null
play.server.http.idleTimeout = 120s
play.server.akka.requestTimeout = 120s

#TODO Move to tenant table
aehsc.client.id="AlertEnterpriseClient"
aehsc.client.secret="oKS+6bX1MkDfStHuCx4UrlBaKailmwCu"

play.application.loader = "com.alnt.platform.application.CustomApplicationLoader"
play {
  modules {
    enabled += "com.alnt.platform.modules.PlatformModule"
    enabled += "com.alnt.assetmgmt.modules.AssetModule"
    enabled += "com.alnt.platform.base.persistence.db.jpa.JPAModule"
    enabled += "com.alnt.platform.base.cache.redis.RedisCacheModule"
	enabled += "com.alnt.platform.core.event.EventSystemModule"
    enabled += "com.alnt.jobserver.platform.base.JobModule"
    enabled += "com.alnt.jobserver.SchedulerModule"
    #enabled += "org.flywaydb.play.PlayModule"
    #enabled += "be.objectify.deadbolt.java.DeadboltModule"
    #enabled += "com.alnt.platform.application.security.authorization.modules.CustomDeadboltHook"
  }
  jpa {
  	master {
		name=master
		schema=${SCHEMA}
		puName=master
	}
	logging {
		name=logging
		schema=${SCHEMA}
		puName=logging
	}
	analytics {
	    name=analytics
	    schema=${SCHEMA}
	    puName=analytics
	}
	events {
	    name=events
	    schema=${SCHEMA_EVT}
	    puName=events
	}   
  }
  eventing {
    topics=${EVENT_TOPICS}
    handler=${EVENT_HANDLER}
	consumer.enabled=true
    activemq {
    	broker.url=${ACTIVEMQ_BROKER_URL}
    	client.id=${ACTIVEMQ_CLIENT_ID}
    	userName=${ACTIVEMQ_USERNAME}
    	encodedkey =${ACTIVEMQ_ENCODEDKEY}
    	isDestinationTopic=${IS_DESTINATION_TOPIC}
    }
  	kafka {
 		bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS}
  		client.id=${KAFKA_CLIENT_ID}
 		acks=1
 		request.timeout.ms=10000
  		queue.buffering.max.messages=10
  		key.serializer="org.apache.kafka.common.serialization.StringSerializer"
  		value.serializer="org.apache.kafka.common.serialization.StringSerializer"
 	
		#Consumer settings		
  		key.deserializer="org.apache.kafka.common.serialization.StringDeserializer"
 		value.deserializer="org.apache.kafka.common.serialization.StringDeserializer"
 		group.id="default_handler"
		
 	}
	azurebus {
    	connection.string=${AZUREBUS_CONNECTION_STRING}
		isDestinationTopic=${IS_DESTINATION_TOPIC}
    }
  }
  cache {
      local {
		syncCache=${LOCAL_METHOD_CACHE_DEF}
      	enabled.entities=${LOCAL_CACHE_ENTITIES}
      }
	  redis {
	  	  enabled.entities=${REDIS_CACHE_ENTITIES}
	  	  bind-default: false
	  	  default-cache: "redis"
		  #https://github.com/KarelCemus/play-redis/blob/2.6.0/doc/20-configuration.md#running-in-different-environments
		  # source property; standalone is default
		  source: standalone #or cluster
		  host:       ${REDIS_HOST}
	      port:       ${REDIS_PORT}
	      database:   ${REDIS_DATABASE}
	      username:       ${REDIS_USERNAME}
	      connectiontimeout:       ${REDIS_CONNECTION_TIMEOUT}
	      encodedkey = ${REDIS_ENCODEDKEY}
	      keystorepath:${REDIS_KEYSTOREPATH}
	      keystorekey:${REDIS_KEYSTOREKEY}
	      truststorepath:${REDIS_TRUSTSOREPATH}
	      truststorekey:${REDIS_TRUSTSOREKEY}
	      ssl : ${REDIS_SSL}


	   }
   }
#  i18n {
#    langs=["en"]
#  }
#  http {
#    filters=be.objectify.deadbolt.java.test.Filters
#    secret {
#      key="Fwk]`dTPGK<eONeYPtEJjwHuuekvtu2U3?S[N>H;gwk`0Z^7?D2v;@ePnsNubNXY"
#    }
#  }
}

#deadbolt {
#  java {
#    cache-user=false
#  }
#}

play.http.errorHandler = "com.alnt.platform.base.exception.handler.ErrorHandler"
play.http.filters = "com.alnt.platform.application.filters.Filters"
play.filters {
	disabled+=play.filters.csrf.CSRFFilter
	enabled +=play.filters.gzip.GzipFilter
	cors {
	  pathPrefixes = ["/api/*", ...]
	  allowedOrigins = ${PLAY_FILTERS_CORS_ALLOWEDORIGINS}
	  allowedHttpMethods = ["GET", "POST"]
	  allowedHttpHeaders = ["Accept"]
	  preflightMaxAge = 3 days
	}
	hosts {
      # Allow requests to example.com, its subdomains, and localhost:9000.
      allowed = ${PLAY_FILTERS_HOSTS}
	}
}

# Number of database connections
db.default {
  driver = ${DB_DEFAULT_DRIVER}
  url = ${DB_DEFAULT_URL}
  username = ${DB_DEFAULT_USERNAME}
  encodedkey = ${DB_DEFAULT_ENCODEDKEY}
  logSql = ${DB_DEFAULT_LOGSQL}
  # Provided for JPA access
  jndiName=DefaultDS
  # Set Hikari to fixed size
  hikaricp.minimumIdle = 1
  hikaricp.maximumPoolSize = ${DB_DEFAULT_CONNECTIONPOOL}
  hikaricp.idleTimeout = 60000
  hikaricp.leakDetectionThreshold=60000
  hikaricp.poolName = AEJobHikariPool
  hikaricp.maxLifetime = 300000
  hikaricp.connectionTimeout = 300000
  hikaricp.autoCommit = true
  hikaricp.transactionIsolation=${DB_DEFAULT_TRANSACTION_ISOLATION}
}

db.logging {
	driver = ${DB_LOGGING_DRIVER}
  	url = ${DB_LOGGING_URL}
  	username = ${DB_LOGGING_USERNAME}
 	 encodedkey = ${DB_LOGGING_ENCODEDKEY}
	 logSql = ${DB_LOGGING_LOGSQL}
  	# Provided for JPA access
  	jndiName=LoggingDS
  	# Set Hikari to fixed size
  	hikaricp.minimumIdle = 1
  	hikaricp.maximumPoolSize = ${DB_LOGGING_CONNECTIONPOOL}
  	hikaricp.idleTimeout = 60000
  	hikaricp.leakDetectionThreshold=60000
  	hikaricp.poolName = AEHikariPoolLogging
  	hikaricp.maxLifetime = 300000
  	hikaricp.connectionTimeout = 300000
  	hikaricp.autoCommit = true
  	hikaricp.transactionIsolation=${DB_LOGGING_TRANSACTION_ISOLATION}
}

db.analytics {
	driver = ${DB_ANALYTICS_DRIVER}
  	url = ${DB_ANALYTICS_URL}
  	username = ${DB_ANALYTICS_USERNAME}
 	encodedkey = ${DB_ANALYTICS_ENCODEDKEY}
	logSql=${DB_ANALYTICS_LOGSQL}
  	# Provided for JPA access
  	jndiName=AnalyticsDS
  	# Set Hikari to fixed size
  	hikaricp.minimumIdle = 1
  	hikaricp.maximumPoolSize = ${DB_ANALYTICS_CONNECTIONPOOL}
  	hikaricp.idleTimeout = 60000
  	hikaricp.leakDetectionThreshold=60000
  	hikaricp.poolName = AEHikariAnalyticsPool
  	hikaricp.maxLifetime = 300000
  	hikaricp.connectionTimeout = 300000
  	hikaricp.autoCommit = true
  	hikaricp.transactionIsolation=${DB_ANALYTICS_TRANSACTION_ISOLATION}
}

db.events {
	driver = ${DB_EVENTS_DRIVER}
  	url = ${DB_EVENTS_URL}
  	username = ${DB_EVENTS_USERNAME}
 	encodedkey = ${DB_EVENTS_ENCODEDKEY}
	logSql=${DB_EVENTS_LOGSQL}
  	# Provided for JPA access
  	jndiName=EventsDS
  	# Set Hikari to fixed size
  	hikaricp.minimumIdle = 1
  	hikaricp.maximumPoolSize = ${DB_EVENTS_CONNECTIONPOOL}
  	hikaricp.idleTimeout = 60000
  	hikaricp.leakDetectionThreshold=60000
  	hikaricp.poolName = AEHikariEventsPool
  	hikaricp.maxLifetime = 300000
  	hikaricp.connectionTimeout = 300000
  	hikaricp.autoCommit = true
  	hikaricp.transactionIsolation=${DB_EVENTS_TRANSACTION_ISOLATION}
}

hibernate.dialect = ${HIBERNATE_DIALECT}
hibernate.hbm2ddl.auto = ${HIBERNATE_HBM2DDL_AUTO}


# Job queue sized to HikariCP connection pool
database.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${DB_DEFAULT_DISPATCHER_THREAD_POOL}
  }
}
database.logging.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${DB_LOGGING_DISPATCHER_THREAD_POOL}
  }
}
database.analytics.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
  fixed-pool-size = ${DB_ANALYTICS_DISPATCHER_THREAD_POOL}
  }
}

assetgetcache.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${ASSET_CACHE_API_THREAD_POOL}
  }
}

translationgetcache.dispatcher{
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${TRANSLATION_THREAD_POOL}
  }
}

job.identity.writer.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${JOB_IDN_WRITER_THREAD_POOL}
  }
}

database.timeline.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${TIMELINE_THREAD_POOL}
  }
}

extidentityapi.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size =  ${EXT_IDENTITY_API_THREAD_POOL}
  }
}

ruleengine.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size =  ${RULE_EXECUTION_THREAD_POOL}
  }
}

workflow.action.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size =  ${WORKFLOW_ACTION_THREAD_POOL}
  }
}
workflow.job.thread.pool= ${WORKFLOW_JOB_THREAD_POOL}

workflow.email.action.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size =  ${WORKFLOW_EMAIL_ACTION_THREAD_POOL}
  }
 }

 accessrequest.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size =  ${ACCESS_REQUEST_THREAD_POOL}
  }
}


akka {
	loglevel = "DEBUG"
	actor {
		default-dispatcher {
			executor = "thread-pool-executor"
			throughput = 1
			thread-pool-executor {
				fixed-pool-size = ${AKKA_ACTOR_THREAD_POOL}
			}
		}
	}
}

job.recon.userevent.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  # Throughput defines the maximum number of messages to be processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
  thread-pool-executor {
    thread-prefix-name = UserEvent
    # The number of threads to keep in the pool, even if they are idle, unless allowCoreThreadTimeOut is set
    core-pool-size = ${DB_EVENTS_CORE_POOL_SIZE}
    allow-core-thread-timeout = false
    # The queue to use for holding tasks before they are executed.
    queue-size = ${DB_EVENTS_QUEUE_SIZE}
    # The maximum number of threads to allow in the pool
    max-pool-size =  ${DB_EVENTS_MAX_POOL_SIZE}
    # When the number of threads is greater than the core, this is the maximum time that excess idle threads
    # will wait for new tasks before terminating
    keep-alive-sec = 30
    # Blocks until all tasks have completed execution after a shutdown request, or the timeout occurs, or the current thread is
    # interrupted, whichever happens first
    shutdown-wait-sec = 60
    # A handler for rejected tasks that runs the rejected task
    # directly in the calling thread of the execute method
    caller-runs-policy = true
  }
}

job.recon.userevent.read.dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  # Throughput defines the maximum number of messages to be processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
  thread-pool-executor {
    thread-prefix-name = UserEventRead
    # The number of threads to keep in the pool, even if they are idle, unless allowCoreThreadTimeOut is set
    core-pool-size = ${DB_EVENTS_READ_CORE_POOL_SIZE}
    allow-core-thread-timeout = false
    # The queue to use for holding tasks before they are executed.
    queue-size = ${DB_EVENTS_READ_QUEUE_SIZE}
    # The maximum number of threads to allow in the pool
    max-pool-size =  ${DB_EVENTS_READ_MAX_POOL_SIZE}
    # When the number of threads is greater than the core, this is the maximum time that excess idle threads
    # will wait for new tasks before terminating
    keep-alive-sec = 30
    # Blocks until all tasks have completed execution after a shutdown request, or the timeout occurs, or the current thread is
    # interrupted, whichever happens first
    shutdown-wait-sec = 60
    # A handler for rejected tasks that runs the rejected task
    # directly in the calling thread of the execute method
    caller-runs-policy = true
  }
}

provisioning-request-dispatcher.dispatcher{
  type = Dispatcher
  executor = "thread-pool-executor"
  throughput = 1
  thread-pool-executor {
    fixed-pool-size = ${REQUEST_PROVISIONING_THREAD_POOL}
  }
}

#Alert Custom
jobserver.service.user = ${JOBSERVER_SERVICE_USER}
jobserver.scheduler.instanceName=${JOBSERVER_SCHEDULER_INSTANCE_NAME}
jobserver.scheduler.startup=${JOBSERVER_SCHEDULER_PRIMARY}
#Interval in Seconds
jobserver.scheduler.stop.checkinterval=5
jobserver.scheduler.stop.threshold=180
jobserver.clustered=${JOBSERVER_CLUSTERED}
jobserver.scheduler.startup=${JOBSERVER_SCHEDULER_PRIMARY}
#Interval in Seconds
jobserver.scheduler.stop.checkinterval=5
jobserver.scheduler.stop.threshold=180
jobserver.driverDelegateClass=${JOBSERVER_DRIVER_DELEGATE}
jobs.create.isolation.level=${JOBS_CREATE_ISOLATION_LEVEL}
jobserver.threadCount=${JOBSERVER_THREADCOUNT}
domain.name=${DOMAIN_NAME}
aes.encryption.passphrase=${AES_ENCRYPTION_PASSPHRASE}
aes.encryption.iterationcount=${AES_ENCRYPTION_ITERATIONCOUNT}
aes.encryption.keylength=${AES_ENCRYPTION_KEYLENGTH}
app.valueof.null="-9999999999"
app.valid.contenttype=["application/json", "multipart/form-data","application/x-www-form-urlencoded"]
timeline.javatype.allowed=${JAVA_OBJECT_TYPES_TIMELINE_MESSAGES}
play.massoperation.max.size=1000
play.maxSizelimitForUpload=${MAX_SIZE_LIMIT_FOR_UPLOAD}
play.jpa.batchSize=10000
recon.job.chunkSize=10000
recon.job.workflow.chunkSize=${WORKFLOW_JOB_CHUNKSIZE}
recon.job.user.recon.chunkSize=${USER_RECON_JOB_CHUNKSIZE}
recon.job.entityupdate.chunkSize=${ENTITY_UPDATE_JOB_CHUNKSIZE}
import.job.chunkSize=${IMPORT_JOB_CHUNKSIZE}

#To Create connector instances at startup and later reuse. Keep value =1 or >1. Keep 0 to turn off cache.
connector.instance=0
auth.token.expiry=${AUTH_TOKEN_EXPIRY}
refresh.token.expiry=${REFRESH_TOKEN_EXPIRY}
application.mode=dev
app.rounddate.validation="true"

#Evolutions TODO: Remove
#db.default.migration.auto=true
#db.default.migration.initOnMigrate=true
#db.default.migration.schemas=["public"]
#db.default.migration.locations=["postgre","common"]

play.evolutions.enabled=${PLAY_EVOLUTIONS_ENABLED}
applyEvolutions.default=${APPLYEVOLUTIONS_DEFAULT}
play.evolutions.autoApply=${PLAY_EVOLUTIONS_AUTOAPPLY}
play.evolutions.schema=${PLAY_EVOLUTIONS_SCHEMA}

#TODO : Move to config_setting
idm.import.limit = ${IDM_IMPORT_LIMIT}
idm.import.mapping.system=${IDM_IMPORT_MAPPING_SYSTEM}

pac4j.security {
  rules = [
    {"/rest-jwt.*" = {
      clients = "ParameterClient"
    }}
    {"/csrf.*" = {
      clients = "AnonymousClient"
    }}
  ]
}

#Translation
translation.disabled=${TRANSLATION_DISABLED}
#Database Time Zone
app.database.timezone=${APP_DATABASE_TIMEZONE}
